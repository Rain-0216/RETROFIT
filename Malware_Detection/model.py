import torch
from torch import nn
import torch.nn.functional as F
class LoRILinear(nn.Module):
    def __init__(self, in_features, out_features, rank=0, lora_alpha=64.0, 
                 train_lora=True, lora_dropout=0.1, mask=None):
        super().__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.rank = rank
        self.lora_alpha = lora_alpha
        self.train_lora = train_lora
        
        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))
        nn.init.normal_(self.lora_A, mean=0.0, std=1.0)
        self.lora_A.requires_grad = False  
        
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))
        nn.init.zeros_(self.lora_B)
        self.lora_A.requires_grad = train_lora
        
        self.dropout = nn.Dropout(lora_dropout) if lora_dropout > 0 else nn.Identity()
        self.scaling = lora_alpha / rank if rank > 0 else 1.0
        self.mask = mask 
        
        for param in self.linear.parameters():
            param.requires_grad = not train_lora

    def forward(self, x):
        result = self.linear(x)
        if not self.train_lora:
            return result
        
        lora_input = self.dropout(x)
        lora_output = lora_input @ self.lora_A.T @ self.lora_B.T * self.scaling
        result += lora_output 
        return result

class LORAMLP(nn.Module):
    def __init__(self, input_size, output_size=2, lora_rank=32, train_lora=True, 
                 lora_alpha=64.0,mask=None):
        super(LORAMLP, self).__init__()
        self.lora_rank = lora_rank
        self.lora_alpha = lora_alpha
        self.encoder = nn.Sequential(
            LoRILinear(input_size, 200, rank=lora_rank, lora_alpha=lora_alpha,
                      train_lora=train_lora),
            nn.ReLU(),
            nn.Dropout(0.5),
            LoRILinear(200, 200, rank=lora_rank, lora_alpha=lora_alpha,
                      train_lora=train_lora),
            nn.ReLU(),
            nn.Dropout(0.5),
        )
        self.classification_head = LoRILinear(
            200, output_size, rank=lora_rank, lora_alpha=lora_alpha,
            train_lora=train_lora)
        self.pred = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.encoder(x)
        feature = x
        x = self.classification_head(x)
        x = self.pred(x)
        return feature, x
    
    def predict_proba(self, x):
        _, mlp_out = self.forward(x)
        return mlp_out

    def predict(self, x):
        self.encoded = self.encoder(x)
        self.out = self.classification_head(self.encoded)
        preds = self.out.max(1)[1]
        return preds

    def get_embed_and_logits(self, x):
        return self.forward(x)
    
class DrebinMLP(nn.Module):
    def __init__(self, input_size, output_size=2):
        super(DrebinMLP, self).__init__()
        
        self.input_size = input_size
        self.encoder = nn.Sequential(
            nn.Linear(input_size, 200),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(200, 200),
            nn.ReLU(),
            nn.Dropout(0.5),
        )
        self.classification_head = nn.Linear(200, output_size)
        self.pred = nn.Softmax(dim=1)


    def forward(self, x):
        x = self.encoder(x)
        feature = x
        x = self.classification_head(x)
        x = self.pred(x)
        return feature, x
    
    def predict_proba(self, x):
        _, mlp_out = self.forward(x)
        return mlp_out

    def predict(self, x):
        self.encoded = self.encoder(x)
        self.out = self.classification_head(self.encoded)
        preds = self.out.max(1)[1]
        return preds
