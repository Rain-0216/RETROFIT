import torch
import torch.nn.functional as F
from typing import Optional, Dict, Union, Mapping
from typing_extensions import Literal

@torch.no_grad()
def _trueclass_confidence(logits: torch.Tensor, y: torch.Tensor, T: float = 1.0) -> torch.Tensor:
    probs = F.softmax(logits / T, dim=-1)
    return probs.gather(dim=-1, index=y.view(-1, 1)).squeeze(-1)  

def _kl_student_to_teacher(
    student_logits: torch.Tensor,
    teacher_logits: torch.Tensor,
    T: float = 1.0,
    reduction: Literal["none", "mean", "sum"] = "mean"
) -> torch.Tensor:
    log_pS = F.log_softmax(student_logits / T, dim=-1)
    pT = F.softmax(teacher_logits / T, dim=-1)
    
    kl_per_sample = F.kl_div(log_pS, pT, reduction="none").sum(dim=-1) * (T **2)
    
    if reduction == "none":
        return kl_per_sample
    elif reduction == "mean":
        return kl_per_sample.mean()
    elif reduction == "sum":
        return kl_per_sample.sum()

def _resolve_per_sample(
    y: torch.Tensor,
    spec: Union[float, int, torch.Tensor, Mapping[int, float]],
    *,
    num_classes: Optional[int] = None,
    default: float = 0.5
) -> torch.Tensor:
    B = y.shape[0]  
    device = y.device  
    
    if isinstance(spec, (float, int)):
        return torch.full((B,), float(spec), device=device, dtype=torch.float32)
    
    if isinstance(spec, torch.Tensor):
        spec = spec.to(device, dtype=torch.float32)
        
        if spec.dim() == 1 and spec.shape[0] == B:
            return spec
        
        if spec.dim() == 1:
            return spec.gather(0, y).clone() 

    
    if isinstance(spec, Mapping):
        if num_classes is not None:
            class_thresholds = torch.full(
                (num_classes,), float(default), device=device, dtype=torch.float32
            )
            for cls_idx, threshold in spec.items():
                cls_idx = int(cls_idx) 
                if cls_idx < 0 or cls_idx >= num_classes:
                    raise ValueError(f"{cls_idx} out of [0, {num_classes-1}]")
                class_thresholds[cls_idx] = float(threshold)
            return class_thresholds.gather(0, y).clone()
        
        else:
            out = torch.empty(B, device=device, dtype=torch.float32)
            
            for unique_cls in torch.unique(y).tolist():
                unique_cls_int = int(unique_cls)
                threshold = float(spec.get(unique_cls_int, default))
                out[y == unique_cls] = threshold
            return out
    

@torch.no_grad()
def _percentile_rank_trueprob(
    logits: torch.Tensor, y: torch.Tensor, calib: Mapping[int, torch.Tensor], T: float = 1.0
) -> torch.Tensor:
    refs_sorted = {int(k): torch.sort(v.flatten()).values for k, v in calib.items()}
    p = _trueclass_confidence(logits, y, T=T)
    ranks = torch.empty_like(p)
    for cls in torch.unique(y).tolist():
        cls = int(cls)
        idx = (y == cls)
        if not idx.any():
            continue
        ref = refs_sorted.get(cls, None)
        if ref is None or ref.numel() == 0:
            ranks[idx] = p[idx]
            continue
        pos = torch.searchsorted(ref, p[idx], right=True)
        ranks[idx] = pos.float() / ref.numel()
    return ranks.clamp_(0.0, 1.0)

def _gate(u: torch.Tensor, mode: Literal["none","sigmoid","softplus"], eps: float = 1e-8) -> torch.Tensor:
    if mode == "none":
        return torch.ones_like(u)
    if mode == "sigmoid":
        return torch.sigmoid(u)
    if mode == "softplus":
        return F.softplus(u)
    raise ValueError(f"unknown gate mode {mode}")

def merge_teacher_loss(
    y: torch.Tensor,                         
    logits_old: torch.Tensor,                  
    logits_new: torch.Tensor,                
    student_logits: Optional[torch.Tensor] = None, 
    *,
    mode: Literal["hard","mixture","hybrid"] = "hard",  
    distill_T: float = 1.0,  
    tau_old: Union[float, torch.Tensor, Mapping[int, float]] = 0.5,
    tau_new: Union[float, torch.Tensor, Mapping[int, float]] = 0.5,
    alpha_gate: Literal["sigmoid","softplus"] = "sigmoid",
    alpha_T: float = 0.5,
    alpha_normalize: bool = True,
    priority: Literal["old","new"] = "old", 
    use_alpha_weights: bool = True,

    use_gate_for_beta: bool = True,
    beta_lambda: float = 5.0,
    beta_clip: float = 1e-3,
    
    band_old: float = 0.05,
    band_new: float = 0.05,
    band_temp: float = 0.25,
    
    reduction: Literal["mean","sum","none"] = "mean",  

    conf_source: Literal["trueprob","cce","rank"] = "trueprob",
    T_old_conf: float = 1.0,
    T_new_conf: float = 1.0,
    cce_pvals_old: Optional[torch.Tensor] = None,
    cce_pvals_new: Optional[torch.Tensor] = None,
    rank_calib_old: Optional[Mapping[int, torch.Tensor]] = None,
    rank_calib_new: Optional[Mapping[int, torch.Tensor]] = None,
) -> Dict[str, torch.Tensor]:

    device = logits_old.device
    B, C   = logits_old.shape
    num_classes = C

    pred_old = logits_old.argmax(dim=-1)
    pred_new = logits_new.argmax(dim=-1)
    old_correct = (pred_old == y)
    new_correct = (pred_new == y)

    if conf_source == "trueprob":
        c0 = _trueclass_confidence(logits_old, y, T=T_old_conf)
        ct = _trueclass_confidence(logits_new, y, T=T_new_conf)
    elif conf_source == "cce":
        c0 = cce_pvals_old.to(device).clamp(0,1)
        ct = cce_pvals_new.to(device).clamp(0,1)
    elif conf_source == "rank":
        c0 = _percentile_rank_trueprob(logits_old, y, rank_calib_old, T=T_old_conf)
        ct = _percentile_rank_trueprob(logits_new, y, rank_calib_new, T=T_new_conf)
    tau0_ps = _resolve_per_sample(
        y=y, spec=tau_old, num_classes=num_classes, default=0.5
    )
    taut_ps = _resolve_per_sample(
        y=y, spec=tau_new, num_classes=num_classes, default=0.5
    )

    m0 = c0 - tau0_ps
    mt = ct - taut_ps

    out: Dict[str, torch.Tensor] = {
        "conf_old": c0, "conf_new": ct,
        "margin_old": m0, "margin_new": mt,
        "pred_old": pred_old, "pred_new": pred_new,
    }

    if mode == "hard":
        with torch.no_grad():
            old_accept = old_correct & (c0 >= tau0_ps)
            new_accept = new_correct & (ct >= taut_ps)
            
            if priority == "old":
                keep_mask = old_accept
                gain_mask = (~keep_mask) & new_accept
            else:
                gain_mask = new_accept
                keep_mask = (~gain_mask) & old_accept
            
            overlap_mask = old_accept & new_accept
            neither_mask = ~(keep_mask | gain_mask)

        out.update({
            "keep_mask": keep_mask, "gain_mask": gain_mask,
            "overlap_mask": overlap_mask, "neither_mask": neither_mask
        })

        if student_logits is not None:
            kl_keep = _kl_student_to_teacher(student_logits, logits_old, T=distill_T, reduction="none")
            kl_gain = _kl_student_to_teacher(student_logits, logits_new, T=distill_T, reduction="none")

            if use_alpha_weights:
                margin_keep = c0 - tau0_ps
                margin_gain = ct - taut_ps
                
                if alpha_gate == "sigmoid":
                    alpha_keep = torch.sigmoid(margin_keep / alpha_T)
                    alpha_gain = torch.sigmoid(margin_gain / alpha_T)
                elif alpha_gate == "softplus":
                    alpha_keep = F.softplus(margin_keep / alpha_T)
                    alpha_gain = F.softplus(margin_gain / alpha_T)
                
                alpha_keep = alpha_keep * keep_mask.float()
                alpha_gain = alpha_gain * gain_mask.float()
                
                if alpha_normalize:
                    eps = 1e-8
                    keep_weight_sum = alpha_keep.sum().clamp_min(eps)
                    keep_sample_count = keep_mask.float().sum().clamp_min(eps)
                    alpha_keep = alpha_keep * (keep_sample_count / keep_weight_sum)
                    
                    gain_weight_sum = alpha_gain.sum().clamp_min(eps)
                    gain_sample_count = gain_mask.float().sum().clamp_min(eps)
                    alpha_gain = alpha_gain * (gain_sample_count / gain_weight_sum)
            else:
                alpha_keep = keep_mask.float()
                alpha_gain = gain_mask.float()

            eps = 1e-8
            keep_loss_sum = (kl_keep * alpha_keep).sum()
            gain_loss_sum = (kl_gain * alpha_gain).sum()
            keep_weight_total = alpha_keep.sum()
            gain_weight_total = alpha_gain.sum()

            if reduction == "mean":
                L_keep = keep_loss_sum / keep_weight_total.clamp_min(eps) if keep_weight_total > eps else torch.tensor(0.0, device=device)
                L_gain = gain_loss_sum / gain_weight_total.clamp_min(eps) if gain_weight_total > eps else torch.tensor(0.0, device=device)
            elif reduction == "sum":
                L_keep = keep_loss_sum if keep_weight_total > eps else torch.tensor(0.0, device=device)
                L_gain = gain_loss_sum if gain_weight_total > eps else torch.tensor(0.0, device=device)
            elif reduction == "none":
                L_keep = kl_keep * alpha_keep
                L_gain = kl_gain * alpha_gain

            L_total = L_keep + L_gain
            out.update({
                "L_keep": L_keep, "L_gain": L_gain, "L_total": L_total,
                "n_keep": keep_mask.sum(), "n_gain": gain_mask.sum(),
                "w_keep_sum": keep_weight_total, "w_gain_sum": gain_weight_total,
                "alpha_keep": alpha_keep, "alpha_gain": alpha_gain
            })
        return out

    if mode == "mixture":
        alpha0 = _gate(m0 / (alpha_T + 1e-8), alpha_gate)
        alphat = _gate(mt / (alpha_T + 1e-8), alpha_gate)
        if alpha_normalize:
            alpha0 = alpha0 / (alpha0.mean().clamp_min(1e-8))
            alphat = alphat / (alphat.mean().clamp_min(1e-8))
        out.update({"alpha_old": alpha0, "alpha_new": alphat})

        if use_gate_for_beta:
            beta = alphat / (alphat + alpha0 + 1e-8)
        if beta_clip is not None and beta_clip > 0:
            beta = beta.clamp(min=beta_clip, max=1.0 - beta_clip)

        p_old = F.softmax(logits_old / distill_T, dim=-1)
        p_new = F.softmax(logits_new / distill_T, dim=-1)
        p_star = beta.unsqueeze(-1) * p_new + (1.0 - beta).unsqueeze(-1) * p_old
        out.update({"beta": beta, "p_star": p_star})

        if student_logits is not None:
            log_pS = F.log_softmax(student_logits / distill_T, dim=-1)
            kl_vec = F.kl_div(log_pS, p_star, reduction="none").sum(dim=-1) * (distill_T ** 2)
            L_kd_mix = kl_vec.mean() if reduction == "mean" else (kl_vec.sum() if reduction == "sum" else kl_vec)
            out.update({"L_kd_mix": L_kd_mix, "kd_vec": kl_vec})
        return out

    if mode == "hybrid":
        alpha0 = _gate(m0 / (alpha_T + 1e-8), alpha_gate)
        alphat = _gate(mt / (alpha_T + 1e-8), alpha_gate)
        if alpha_normalize:
            alpha0 = alpha0 / (alpha0.mean().clamp_min(1e-8))
            alphat = alphat / (alphat.mean().clamp_min(1e-8))
        out.update({"alpha_old": alpha0, "alpha_new": alphat})

        def sig(x, t): return torch.sigmoid(x / t)
        old_hard = sig(m0 - band_old, band_temp)
        new_hard = sig(mt - band_new, band_temp)
        overlap = sig(band_old - m0.abs(), band_temp) * sig(band_new - mt.abs(), band_temp)
        old_hard = old_hard * old_correct.float()
        new_hard = new_hard * new_correct.float()

        Z = old_hard + new_hard + overlap + 1e-8
        wK = old_hard * (1 - overlap) / Z
        wG = new_hard * (1 - overlap) / Z
        wO = overlap / Z

        beta = alphat / (alphat + alpha0 + 1e-8)
        if beta_clip is not None and beta_clip > 0:
            beta = beta.clamp(min=beta_clip, max=1.0 - beta_clip)
        p_old = F.softmax(logits_old / distill_T, dim=-1)
        p_new = F.softmax(logits_new / distill_T, dim=-1)
        p_star = beta.unsqueeze(-1) * p_new + (1.0 - beta).unsqueeze(-1) * p_old

        out.update({"wK": wK, "wG": wG, "wO": wO, "beta": beta, "p_star": p_star})

        if student_logits is not None:
            KL_S_old = _kl_student_to_teacher(student_logits, logits_old, T=distill_T, reduction="none")
            KL_S_new = _kl_student_to_teacher(student_logits, logits_new, T=distill_T, reduction="none")
            log_pS = F.log_softmax(student_logits / distill_T, dim=-1)
            KL_S_mix = F.kl_div(log_pS, p_star, reduction="none").sum(dim=-1) * (distill_T ** 2)

            L_K = (wK * KL_S_old)
            L_G = (wG * KL_S_new)
            L_O = (wO * KL_S_mix)

            if reduction == "mean":
                L_K, L_G, L_O = L_K.mean(), L_G.mean(), L_O.mean()
                L_total = L_K + L_G + L_O
            elif reduction == "sum":
                L_K, L_G, L_O = L_K.sum(), L_G.sum(), L_O.sum()
                L_total = L_K + L_G + L_O
            else:
                L_total = L_K + L_G + L_O

            out.update({"L_K": L_K, "L_G": L_G, "L_O": L_O, "L_total": L_total})
        return out

