import os
import numpy as np
import torch
import torch.nn.functional as F
from torch.utils import data

from model import LORAMLP
from retrofit import *
from utils import *
from args import parse_arguments

os.environ['PYTHONHASHSEED'] = '42'

def train_args(train_args):
    train_args.batch_size = 64
    train_args.lr = 0.0001
    train_args.epochs = 50
    train_args.seed = 42
    train_args.rank = 512
    train_args.device = torch.device("cpu")
    return train_args


def merge_args(merge_args):
    merge_args.sigmoid_bias = 2
    merge_args.learning_rate = 0.1
    merge_args.l1_strength = merge_args.learning_rate * 1e-5
    merge_args.grp_strength = merge_args.learning_rate * 1e-5
    merge_args.num_train_epochs = 50
    merge_args.sparsity = 0.01
    merge_args.ce_coef = 0.5
    merge_args.kg_coef = 0.5
    return merge_args


def hard_gain_args(merge_args):
    hard_gain_params = {
        "mode": getattr(merge_args, "merge_mode", "mixture"),
        "distill_T": getattr(merge_args, "kd_T", 1.0),
        "tau_old": getattr(merge_args, "tau_old", 0.5),
        "tau_new": getattr(merge_args, "tau_new", 0.5),
        "alpha_gate": getattr(merge_args, "alpha_gate", "sigmoid"),
        "alpha_T": getattr(merge_args, "alpha_T", 0.5),
        "alpha_normalize": getattr(merge_args, "alpha_normalize", True),
        "priority": getattr(merge_args, "priority", "old"),
        "use_alpha_weights": getattr(merge_args, "use_alpha_weights", False),
        "use_gate_for_beta": True,
        "beta_lambda": 5.0,
        "beta_clip": 1e-3,
        "band_old": 0.05,
        "band_new": 0.05,
        "band_temp": 0.25,
        "reduction": "mean"
    }
    return hard_gain_params

def load_pretrained_weights_to_lori(pretrained_model, lori_model):
    pretrained_dict = pretrained_model.state_dict()
    lori_dict = lori_model.state_dict()
    pretrained_dict = {
        k: v for k, v in pretrained_dict.items()
        if 'lora_A' not in k and k in lori_dict and lori_dict[k].shape == v.shape
    }
    lori_dict.update(pretrained_dict)
    lori_model.load_state_dict(lori_dict, strict=False)

def train_warm_classifier(old_model, X_train, y_train, args):
    seed = args.seed
    rank = args.rank
    device = args.device
    lr = args.lr
    epochs = args.epochs
    batch_size = args.batch_size
    set_reproducible_seed(seed)
    torch.manual_seed(seed)
    classifier = LORAMLP(
        input_size=X_train.shape[1],
        lora_rank=rank,
        train_lora=True
    ).to(device)
    load_pretrained_weights_to_lori(old_model, classifier)

    X_train_tensor = torch.from_numpy(X_train).float().to(device)
    y_train_tensor = torch.from_numpy(y_train).long().to(device)
    y_train_binary = np.array([1 if item != 0 else 0 for item in y_train])
    y_train_binary_cat_tensor = torch.from_numpy(to_categorical(y_train_binary)).float().to(device)
    weight_tensor = torch.ones(X_train.shape[0]).to(device)

    train_iter = data.DataLoader(
        data.TensorDataset(X_train_tensor, y_train_tensor, y_train_binary_cat_tensor, weight_tensor),
        batch_size=batch_size, shuffle=True
    )

    trainable_params = [p for n, p in classifier.named_parameters() if 'lora_B' in n and p.requires_grad]
    optimizer = torch.optim.Adam(trainable_params, lr=lr, weight_decay=0.001)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)

    for epoch in range(epochs):
        losses = AverageMeter()
        classifier.train()
        for idx, (x_batch, y_batch, y_cat_batch, weight_batch) in enumerate(train_iter):
            x_batch = x_batch.to(device)
            y_cat_batch = y_cat_batch.to(device)
            weight_batch = weight_batch.to(device)
            y_pred = classifier.predict_proba(x_batch)[:, 1]
            target = y_cat_batch[:, 1]
            loss_ele = F.binary_cross_entropy(y_pred, target, reduction='none')
            loss = (loss_ele * weight_batch).mean()

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            losses.update(loss.item(), x_batch.size(0))
        scheduler.step(losses.avg)

    return classifier

def train_classifier(X_train, y_train, args):
    seed = args.seed
    rank = args.rank
    device = args.device
    lr = args.lr
    epochs = args.epochs
    batch_size = args.batch_size
    set_reproducible_seed(seed)
    torch.manual_seed(seed)
    classifier = LORAMLP(
        input_size=X_train.shape[1],
        output_size=2,
        lora_rank=rank,
        train_lora=False
    ).to(device)

    X_train_tensor = torch.from_numpy(X_train).float().to(device)
    y_train_tensor = torch.from_numpy(y_train).long().to(device)
    y_train_binary = np.array([1 if item != 0 else 0 for item in y_train])
    y_train_binary_cat_tensor = torch.from_numpy(to_categorical(y_train_binary)).float().to(device)
    weight_tensor = torch.ones(X_train.shape[0]).to(device)

    train_iter = data.DataLoader(
        data.TensorDataset(X_train_tensor, y_train_tensor, y_train_binary_cat_tensor, weight_tensor),
        batch_size=batch_size, shuffle=True
    )

    trainable_params = [p for n, p in classifier.named_parameters() if p.requires_grad]
    optimizer = torch.optim.Adam(trainable_params, lr=lr, weight_decay=0.0001)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True, min_lr=1e-6)

    for epoch in range(epochs):
        losses = AverageMeter()
        classifier.train()
        for idx, (x_batch, y_batch, y_cat_batch, weight_batch) in enumerate(train_iter):
            x_batch = x_batch.to(device)
            y_cat_batch = y_cat_batch.to(device)
            weight_batch = weight_batch.to(device)
            y_pred = classifier.predict_proba(x_batch)[:, 1]
            target = y_cat_batch[:, 1]
            loss_ele = F.binary_cross_entropy(y_pred, target, reduction='none')
            loss = (loss_ele * weight_batch).mean()

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            losses.update(loss.item(), x_batch.size(0))
        scheduler.step(losses.avg)

    return classifier

if __name__ == '__main__':
    feature_name = 'transcend'
    datasets = ['2015', '2016', '2017', '2018']
    obj_flag = True
    sampler = 'unc'

    train_args = train_args(parse_arguments())
    merge_args = merge_args(parse_arguments())
    hard_gain_params = hard_gain_args(merge_args)

    warm = 'lori_fft'
    merge = 'ls_w_hard'
    year = '2014'
    device = train_args.device
    r = train_args.rank
    train_data = np.load(f'data/train_data_{year}.npz', allow_pickle=obj_flag)
    X_base_train = train_data['X']
    y_base_train = train_data['y']

    base_model = train_classifier(X_base_train, y_base_train, train_args)
    torch.save(base_model, f'_{sampler}/{feature_name}/{warm}/{merge}/{year}.model')
    torch.save(base_model, f'_{sampler}/{feature_name}/{warm}/{merge}/new_{year}.model')
    torch.save(base_model, f'_{sampler}/{feature_name}/{warm}/{merge}/merge_{year}.model')

    for year in datasets:
        test_data = np.load(f'data/test_data_{year}.npz', allow_pickle=obj_flag)
        test_X = test_data['X']
        test_y = test_data['y']
        test_X = torch.from_numpy(test_X).float()
        test_y = torch.from_numpy(test_y).long()
        testloader = data.DataLoader(data.TensorDataset(test_X, test_y), batch_size=train_args.batch_size, shuffle=False)

        pre_year = int(year) - 1
        pre_check = str(pre_year)
        old_path = f'_{sampler}/{feature_name}/{warm}/{merge}/merge_{pre_check}.model'
        old_model = torch.load(old_path).to(device)
        merge_model = LORAMLP(
            input_size=X_base_train.shape[1],
            lora_rank=r,
            train_lora=True
        ).to(device)
        load_pretrained_weights_to_lori(old_model, merge_model)

        train_data = np.load(f'data/train_data_{year}.npz', allow_pickle=obj_flag)
        X = train_data['X']
        y = train_data['y']
        
        new_model = train_warm_classifier(old_model, X, y, train_args)
        torch.save(new_model, f'_{sampler}/{feature_name}/{warm}/{merge}/new_{year}.model')

        X = torch.from_numpy(X).float()
        y = torch.from_numpy(y).long()
        train_iter = data.DataLoader(data.TensorDataset(X, y), batch_size=train_args.batch_size, shuffle=True)

        retrofit_merger = Retrofit(merge_model, old_model, new_model, hard_gain_params, train_args, merge_args)
        merge_model = retrofit_merger.train_mask(train_iter)
        torch.save(merge_model, f"_{sampler}/{feature_name}/{warm}/{merge}/merge_{year}.model")
