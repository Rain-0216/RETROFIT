import torch.nn as nn
import torch
from utils import *
from keep_gain_loss import merge_teacher_loss

class Retrofit(nn.Module):
    def __init__(self, model, old_model, new_model, hard_gain_params, args, merge_args):
        super(Retrofit, self).__init__()
        self.device = args.device
        self.model = model.to(self.device)
        self.old_model = old_model.to(self.device)
        self.new_model = new_model.to(self.device)
        self.args = args
        self.merge_args = merge_args
        self.model.train()
        self.new_model.eval()
        self.old_model.eval()
        self.classifier_head = model.classification_head.to(self.device)

        self.lora_A_names = [name for name, _ in self.model.named_parameters() if 'lora_A' in name]
        self.lora_B_names = [name for name, _ in self.model.named_parameters() if 'lora_B' in name]

        for name, param in self.model.named_parameters():
            if name in self.lora_B_names:
                param.requires_grad = True
            else:
                param.requires_grad = False  

        self.hard_gain_params = hard_gain_params

        self.trainable_params = {name: param for name, param in self.model.named_parameters() 
                                if param.requires_grad}
        self.trainable_name = list(self.trainable_params.keys())

        self.create_task_vector()
        self.lora_B_mask = self.create_lora_B_mask()
        
    def create_task_vector(self):
        self.task_vectors = []
        for counter in range(len(self.trainable_name)):
            pretensor = None
            for pre_n, pre_p in self.old_model.named_parameters():
                if pre_n == self.trainable_name[counter]:
                    pretensor = pre_p
                    break
            
            finetensor = None
            for fine_n, fine_p in self.new_model.named_parameters():
                if fine_n == self.trainable_name[counter]:
                    finetensor = fine_p
                    break
            
            if pretensor is not None and finetensor is not None:
                self.task_vectors += [(finetensor - pretensor).detach()]
            
    def create_lora_B_mask(self):
        abs_task_vector = []
        for p in self.task_vectors:
            abs_task_vector.append(torch.abs(p).view(-1))
        abs_task_vector = torch.cat(abs_task_vector)

        k = int(self.merge_args.sparsity * abs_task_vector.numel())
        if k <= 0:  
            k = 1
        if k >= abs_task_vector.numel():  
            k = abs_task_vector.numel()

        topk_tv_idx = torch.topk(abs_task_vector, k, largest=True, sorted=False).indices
        selected = torch.zeros_like(abs_task_vector, dtype=torch.bool)
        selected[topk_tv_idx] = True

        masks, offset = [], 0
        for p in self.task_vectors:
            numel = p.numel()
            mask = torch.full_like(p, -self.merge_args.sigmoid_bias,
                                device=self.device, requires_grad=False)
            sel = selected[offset:offset+numel].view_as(p)
            mask[sel] = self.merge_args.sigmoid_bias
            mask.requires_grad_(True)
            masks.append(mask)
            offset += numel

        return masks


    def initialize_model(self):
        for a_name in self.lora_A_names:
            weight_name = a_name.replace('lora_A', 'linear.weight')
            pretensor = None
            for pre_n, pre_p in self.old_model.named_parameters():
                if pre_n == weight_name:
                    pretensor = pre_p.to(self.device)
                    break
            if pretensor is not None:
                with torch.no_grad():
                    self.model.get_parameter(weight_name).copy_(pretensor)
    
        for name in self.lora_A_names:
            pretensor = None
            for pre_n, pre_p in self.old_model.named_parameters():
                if pre_n == name:
                    pretensor = pre_p.to(self.device)
                    break
            if pretensor is not None:
                with torch.no_grad():
                    self.model.get_parameter(name).copy_(pretensor)
        
        for name in self.lora_B_names:
            pretensor = None
            for pre_n, pre_p in self.old_model.named_parameters():
                if pre_n == name:
                    pretensor = pre_p.to(self.device)
                    break
            if pretensor is not None:
                with torch.no_grad():
                    self.model.get_parameter(name).copy_(pretensor)
    

    def merge_model(self):
        sigmoid = torch.nn.Sigmoid()
        for a_name in self.lora_A_names:
            b_name = a_name.replace('lora_A', 'lora_B')
            mask_idx = self.lora_B_names.index(b_name)
            b_mask = self.lora_B_mask[mask_idx]  
            b_frac = sigmoid(b_mask).to(self.device) 
            
            old_A = self.old_model.get_parameter(a_name).clone().to(self.device) 
            old_B = self.old_model.get_parameter(b_name).clone().to(self.device) * (1 - b_frac)
            old_linear_weight = self.old_model.get_parameter(a_name.replace('lora_A', 'linear.weight')).to(self.device)
            new_A = self.new_model.get_parameter(a_name).clone().to(self.device) 
            new_B = self.new_model.get_parameter(b_name).clone().to(self.device) * b_frac
            scaling = self.model.lora_alpha / self.model.lora_rank
            old_BA = old_B @ old_A * scaling
            new_BA = new_B @ new_A * scaling
            
            target_weight = old_linear_weight + old_BA + new_BA
            current_weight = self.model.get_parameter(a_name.replace('lora_A', 'linear.weight'))
            weight_delta = target_weight - current_weight 
            current_weight.add_(weight_delta) 
            lora_B_param = self.model.get_parameter(b_name)
            with torch.no_grad():  
                lora_B_param.copy_(torch.zeros_like(lora_B_param)) 
            lora_B_param.requires_grad_(True)
        
        return self.model

    def sparse_group_lasso(self, M, lambda1=1e-4, lambda_grp=1e-3, size_norm=True):
        device = self.device
        l1 = M.abs().sum()
        
        group_norms = M.norm(p=2, dim=0)  

        if size_norm:
            group_size = M.shape[0]  
            w = torch.as_tensor(group_size ** -0.5, device=device) 
        else:
            w = torch.as_tensor(1.0, device=device) 
        
        grp = w * group_norms.sum()  
        
        return lambda1 * l1 + lambda_grp * grp
    
        
    def train_mask(self, dataloader):
        device = self.device
        loss_fct = torch.nn.CrossEntropyLoss()
        epoch_losses = []

        self.optimizer = torch.optim.AdamW(
            params=[{'params': self.lora_B_mask, 'lr': self.merge_args.learning_rate}],
            weight_decay=1e-4
        )
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, 
            T_max=self.merge_args.num_train_epochs,
            eta_min=1e-3
        )

        self.ce_coef = self.merge_args.ce_coef
        self.kg_coef = self.merge_args.kg_coef
        
        for epoch in range(self.merge_args.num_train_epochs):
            self.optimizer.zero_grad()  
            epoch_loss = 0.0
            epoch_reg_loss = 0.0   
            epoch_ce_loss = 0.0 
            epoch_kg_loss = 0.0 
            model = self.merge_model()
            
            batch_grad_norms = []  
            
            for batch_idx, data in enumerate(dataloader):
                inputs = data[0].to(device)
                labels = data[1].to(device)
                

                old_feature, _ = self.old_model(inputs)
                logits_old = self.old_model.classification_head(old_feature)
                
                new_feature, _ = self.new_model(inputs)
                logits_new = self.new_model.classification_head(new_feature)
                
                student_feature, _ = self.model(inputs)
                student_logits = self.model.classification_head(student_feature)
                
                loss = torch.tensor(0.0, device=self.device)
                raw_ce = torch.tensor(0.0, device=self.device)
                raw_kg = torch.tensor(0.0, device=self.device)

                raw_ce = loss_fct(student_logits, labels) 
                ce_loss = self.ce_coef * raw_ce  
                gain_loss_dict = merge_teacher_loss(
                    y=labels,
                    logits_old=logits_old,
                    logits_new=logits_new,
                    student_logits=student_logits,** self.hard_gain_params
                )
                if self.hard_gain_params["mode"] == 'hard':
                    raw_kg = gain_loss_dict["L_keep"] + gain_loss_dict["L_gain"] 
                elif self.hard_gain_params["mode"] == "mixture":
                    raw_kg = gain_loss_dict["L_kd_mix"] 
                else:
                    raw_kg = gain_loss_dict["L_total"] 
                
                kg_loss = self.kg_coef * raw_kg  
                loss = ce_loss + kg_loss
                
                reg = torch.zeros((), device=device)
                for mask in self.lora_B_mask:
                    processed_mask = torch.sigmoid(mask)
                    reg += self.sparse_group_lasso(
                        processed_mask,
                        lambda1=self.merge_args.l1_strength,
                        lambda_grp=self.merge_args.grp_strength,
                        size_norm=True  
                    )
                loss = ce_loss + kg_loss + reg

                epoch_loss += loss.item()
                epoch_reg_loss += reg.item()
                epoch_ce_loss += ce_loss.item()
                epoch_kg_loss += kg_loss.item()

                loss.backward(retain_graph=True)  
                grad_norm = torch.norm(self.lora_B_mask[0].grad).item() if self.lora_B_mask[0].grad is not None else 0.0
                batch_grad_norms.append(grad_norm)
                
            self.optimizer.step()
            self.scheduler.step()
            self.optimizer.zero_grad()

            epoch_loss /= len(dataloader)
            epoch_reg_loss /= len(dataloader)
            epoch_losses.append(epoch_loss)
            self.initialize_model()

        self.initialize_model()
        merge_model = self.merge_model()
        return merge_model
